{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stock Market Return Prediction(Time Series Problem)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:08.417085Z",
     "start_time": "2022-01-28T19:21:51.123612Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from keras.layers import Conv1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:11.783536Z",
     "start_time": "2022-01-28T19:23:11.436799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>GSPC.Open</th>\n",
       "      <th>GSPC.High</th>\n",
       "      <th>GSPC.Low</th>\n",
       "      <th>GSPC.Close</th>\n",
       "      <th>GSPC.Volume</th>\n",
       "      <th>GSPC.Adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-02</td>\n",
       "      <td>92.059998</td>\n",
       "      <td>93.540001</td>\n",
       "      <td>91.790001</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>8050000.0</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-05</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>94.250000</td>\n",
       "      <td>92.529999</td>\n",
       "      <td>93.459999</td>\n",
       "      <td>11490000.0</td>\n",
       "      <td>93.459999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-06</td>\n",
       "      <td>93.459999</td>\n",
       "      <td>93.809998</td>\n",
       "      <td>92.129997</td>\n",
       "      <td>92.820000</td>\n",
       "      <td>11460000.0</td>\n",
       "      <td>92.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-07</td>\n",
       "      <td>92.820000</td>\n",
       "      <td>93.379997</td>\n",
       "      <td>91.930000</td>\n",
       "      <td>92.629997</td>\n",
       "      <td>10010000.0</td>\n",
       "      <td>92.629997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-08</td>\n",
       "      <td>92.629997</td>\n",
       "      <td>93.470001</td>\n",
       "      <td>91.989998</td>\n",
       "      <td>92.680000</td>\n",
       "      <td>10670000.0</td>\n",
       "      <td>92.680000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Index   GSPC.Open  GSPC.High   GSPC.Low  GSPC.Close  GSPC.Volume  \\\n",
       "0  1970-01-02  92.059998  93.540001  91.790001   93.000000    8050000.0   \n",
       "1  1970-01-05  93.000000  94.250000  92.529999   93.459999   11490000.0   \n",
       "2  1970-01-06  93.459999  93.809998  92.129997   92.820000   11460000.0   \n",
       "3  1970-01-07  92.820000  93.379997  91.930000   92.629997   10010000.0   \n",
       "4  1970-01-08  92.629997  93.470001  91.989998   92.680000   10670000.0   \n",
       "\n",
       "   GSPC.Adjusted  \n",
       "0      93.000000  \n",
       "1      93.459999  \n",
       "2      92.820000  \n",
       "3      92.629997  \n",
       "4      92.680000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sp500.csv', delimiter = ' ', header=None, skiprows=1, names=[\"Index \", \"GSPC.Open\", \"GSPC.High\", \"GSPC.Low\", \"GSPC.Close\", \"GSPC.Volume\", \"GSPC.Adjusted\"],engine='python')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:12.609501Z",
     "start_time": "2022-01-28T19:23:12.512658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index            0\n",
       "GSPC.Open        0\n",
       "GSPC.High        0\n",
       "GSPC.Low         0\n",
       "GSPC.Close       0\n",
       "GSPC.Volume      0\n",
       "GSPC.Adjusted    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of missing values in the dataset\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Prediction Task\n",
    "\n",
    "This project aims to have good forecasts of the future price of the S&P500 index so that profitable orders can be placed on time. Below we describe a variable, calculated with the quotes data, that can be seen as an indicator (a value) of the tendency in the next k days. The value of this indicator should be related to the confidence we have that the target\n",
    "margin p will be attainable in the next k days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:15.247987Z",
     "start_time": "2022-01-28T19:23:14.612852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>GSPC.Open</th>\n",
       "      <th>GSPC.High</th>\n",
       "      <th>GSPC.Low</th>\n",
       "      <th>GSPC.Close</th>\n",
       "      <th>GSPC.Volume</th>\n",
       "      <th>GSPC.Adjusted</th>\n",
       "      <th>Daily_average_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-02</td>\n",
       "      <td>92.059998</td>\n",
       "      <td>93.540001</td>\n",
       "      <td>91.790001</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>8050000.0</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>92.776667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-05</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>94.250000</td>\n",
       "      <td>92.529999</td>\n",
       "      <td>93.459999</td>\n",
       "      <td>11490000.0</td>\n",
       "      <td>93.459999</td>\n",
       "      <td>93.413333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-06</td>\n",
       "      <td>93.459999</td>\n",
       "      <td>93.809998</td>\n",
       "      <td>92.129997</td>\n",
       "      <td>92.820000</td>\n",
       "      <td>11460000.0</td>\n",
       "      <td>92.820000</td>\n",
       "      <td>92.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-07</td>\n",
       "      <td>92.820000</td>\n",
       "      <td>93.379997</td>\n",
       "      <td>91.930000</td>\n",
       "      <td>92.629997</td>\n",
       "      <td>10010000.0</td>\n",
       "      <td>92.629997</td>\n",
       "      <td>92.646665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-08</td>\n",
       "      <td>92.629997</td>\n",
       "      <td>93.470001</td>\n",
       "      <td>91.989998</td>\n",
       "      <td>92.680000</td>\n",
       "      <td>10670000.0</td>\n",
       "      <td>92.680000</td>\n",
       "      <td>92.713333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Index   GSPC.Open  GSPC.High   GSPC.Low  GSPC.Close  GSPC.Volume  \\\n",
       "0  1970-01-02  92.059998  93.540001  91.790001   93.000000    8050000.0   \n",
       "1  1970-01-05  93.000000  94.250000  92.529999   93.459999   11490000.0   \n",
       "2  1970-01-06  93.459999  93.809998  92.129997   92.820000   11460000.0   \n",
       "3  1970-01-07  92.820000  93.379997  91.930000   92.629997   10010000.0   \n",
       "4  1970-01-08  92.629997  93.470001  91.989998   92.680000   10670000.0   \n",
       "\n",
       "   GSPC.Adjusted  Daily_average_price  \n",
       "0      93.000000            92.776667  \n",
       "1      93.459999            93.413333  \n",
       "2      92.820000            92.919998  \n",
       "3      92.629997            92.646665  \n",
       "4      92.680000            92.713333  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Daily_average_price'] = (df['GSPC.High']+df['GSPC.Low']+df['GSPC.Close'])/3\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:15.375954Z",
     "start_time": "2022-01-28T19:23:15.367956Z"
    }
   },
   "outputs": [],
   "source": [
    "def T_index(n_days=120, margin=0.025):\n",
    "    T_list = []\n",
    "    for i in df.index:\n",
    "        T = 0\n",
    "        Ci = df['GSPC.Close'].iloc[i]\n",
    "        for j in range (i + 1, n_days + 1):\n",
    "            variation = (df['Daily_average_price'].iloc[j] - Ci) / Ci\n",
    "            if (variation > margin) or (variation < -margin) :\n",
    "                T = T + variation\n",
    "        T_list.append(T)\n",
    "    return(T_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:16.437528Z",
     "start_time": "2022-01-28T19:23:16.188253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.000000    11508\n",
       "-1.132061        2\n",
       "-6.501096        2\n",
       "-0.220323        2\n",
       "-3.395769        1\n",
       "             ...  \n",
       "-4.646347        1\n",
       " 0.828822        1\n",
       "-5.059725        1\n",
       "-4.910865        1\n",
       "-2.369302        1\n",
       "Name: T_index, Length: 112, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['T_index'] = T_index()\n",
    "df['T_index'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea of the variable T is to signal k-days periods that have\n",
    "several days with average daily prices clearly above the target variation. High\n",
    "positive values of T mean that there are several average daily prices that are\n",
    "p% higher than today’s close. Such situations are good indications of potential\n",
    "opportunities to issue a buy order, as we have good expectations that the prices\n",
    "will rise. On the other hand, highly negative values of T suggest sell actions,\n",
    "given the prices will probably decline. Values around zero can be caused by\n",
    "periods with “flat” prices or by conflicting positive and negative variations\n",
    "that cancel each other.\n",
    "\n",
    "## Defining the Predictors\n",
    "\n",
    "We have defined an indicator (T) that summarizes the behavior of the price time series in the next k days. We are approximating the future behavior (f), by our indicator T. We now have to decide on how we will describe the recent prices pattern (p in the description above). Instead of using again a single indicator to describe these recent dynamics, we will use several thechnical indicators, trying to capture different properties of the price time series to facilitate the forecasting task. The technical indicators which were used in this task are the Average True Value, Directional Movement Index, Parabolic SAR, Moving Average, Arms Ease of Movement, Volatility OHC, Percentage Change and MACD oscillator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:20.897430Z",
     "start_time": "2022-01-28T19:23:17.999702Z"
    }
   },
   "outputs": [],
   "source": [
    "# macd indicator\n",
    "macd = df.ta.macd(close='GSPC.Close', append=False)\n",
    "df['MACD'] = macd['MACD_12_26_9']\n",
    "\n",
    "# atr indicator\n",
    "atr = df.ta.atr(high='GSPC.High', low='GSPC.LOW', close='GSPC.Close', length=10, append=False)\n",
    "df['ATR'] = atr\n",
    "\n",
    "# adx indicator\n",
    "adx = df.ta.adx(high='GSPC.High', low='GSPC.LOW', close='GSPC.Close', length=10, append=False)\n",
    "df['ADX'] = adx['ADX_10']\n",
    "\n",
    "# psar indicator\n",
    "psar = df.ta.psar(high='GSPC.High', low='GSPC.LOW', close='GSPC.Close', append=False)\n",
    "df['PSAR'] = psar['PSARl_0.02_0.2'].fillna(psar['PSARs_0.02_0.2'])\n",
    "\n",
    "# evm indicator\n",
    "def EVM(data, ndays): \n",
    " dm = ((data['GSPC.High'] + data['GSPC.Low'])/2) - ((data['GSPC.High'].shift(1) + data['GSPC.Low'].shift(1))/2)\n",
    " br = (data['GSPC.Volume'] / 100000000) / ((data['GSPC.High'] - data['GSPC.Low']))\n",
    " EVM = dm / br \n",
    " EVM_MA = pd.Series(EVM.rolling(ndays).mean(), name = 'EVM')\n",
    " return EVM_MA\n",
    "\n",
    "emv = EVM(pd.read_csv('sp500.csv', delimiter = ' ', header=None, skiprows=1, names=[\"Index \", \"GSPC.Open\", \"GSPC.High\", \"GSPC.Low\", \"GSPC.Close\", \"GSPC.Volume\", \"GSPC.Adjusted\"],engine='python'), 10)\n",
    "df['EMV'] = emv\n",
    "\n",
    "# runMean indicator\n",
    "def runMean (column_series, window_size):\n",
    "    windows = column_series.rolling(window_size)\n",
    "    # Get the window of series\n",
    "    # of observations of specified window size\n",
    "    windows = column_series.rolling(window_size)\n",
    "  \n",
    "    # Create a series of moving\n",
    "    # averages of each window\n",
    "    moving_averages = windows.mean()\n",
    "  \n",
    "    # Convert pandas series back to list\n",
    "    moving_averages_list = moving_averages.tolist()\n",
    "  \n",
    "    # Remove null entries from the list\n",
    "    final_list = moving_averages_list[window_size - 1:]\n",
    "  \n",
    "    return(final_list)\n",
    "    \n",
    "runMean = runMean(df['GSPC.Close'], 10)\n",
    "runMean.extend([0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "df['runMean'] = runMean\n",
    "\n",
    "# Volat indicator\n",
    "volat = np.sqrt(1 / 10 * pd.DataFrame.rolling(0.5 * np.log(df.loc[:, 'GSPC.High'] / df.loc[:, 'GSPC.Low']) ** 2 - (2 * np.log(2) - 1) * np.log(df.loc[:, 'GSPC.Close'] / df.loc[:, 'GSPC.Open']) ** 2, window=10).sum())\n",
    "df['Volat'] = volat\n",
    "\n",
    "# Delt indicator\n",
    "df['DELT'] = df['GSPC.Close'].pct_change(periods=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:20.985121Z",
     "start_time": "2022-01-28T19:23:20.953126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>GSPC.Open</th>\n",
       "      <th>GSPC.High</th>\n",
       "      <th>GSPC.Low</th>\n",
       "      <th>GSPC.Close</th>\n",
       "      <th>GSPC.Volume</th>\n",
       "      <th>GSPC.Adjusted</th>\n",
       "      <th>Daily_average_price</th>\n",
       "      <th>T_index</th>\n",
       "      <th>MACD</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ADX</th>\n",
       "      <th>PSAR</th>\n",
       "      <th>EMV</th>\n",
       "      <th>runMean</th>\n",
       "      <th>Volat</th>\n",
       "      <th>DELT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-02</td>\n",
       "      <td>92.059998</td>\n",
       "      <td>93.540001</td>\n",
       "      <td>91.790001</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>8050000.0</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>92.776667</td>\n",
       "      <td>-11.200394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.393999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-05</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>94.250000</td>\n",
       "      <td>92.529999</td>\n",
       "      <td>93.459999</td>\n",
       "      <td>11490000.0</td>\n",
       "      <td>93.459999</td>\n",
       "      <td>93.413333</td>\n",
       "      <td>-11.686674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.185999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-06</td>\n",
       "      <td>93.459999</td>\n",
       "      <td>93.809998</td>\n",
       "      <td>92.129997</td>\n",
       "      <td>92.820000</td>\n",
       "      <td>11460000.0</td>\n",
       "      <td>92.820000</td>\n",
       "      <td>92.919998</td>\n",
       "      <td>-11.008798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.805000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-07</td>\n",
       "      <td>92.820000</td>\n",
       "      <td>93.379997</td>\n",
       "      <td>91.930000</td>\n",
       "      <td>92.629997</td>\n",
       "      <td>10010000.0</td>\n",
       "      <td>92.629997</td>\n",
       "      <td>92.646665</td>\n",
       "      <td>-10.805747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.506000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-08</td>\n",
       "      <td>92.629997</td>\n",
       "      <td>93.470001</td>\n",
       "      <td>91.989998</td>\n",
       "      <td>92.680000</td>\n",
       "      <td>10670000.0</td>\n",
       "      <td>92.680000</td>\n",
       "      <td>92.713333</td>\n",
       "      <td>-10.859265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.1108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.238000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Index   GSPC.Open  GSPC.High   GSPC.Low  GSPC.Close  GSPC.Volume  \\\n",
       "0  1970-01-02  92.059998  93.540001  91.790001   93.000000    8050000.0   \n",
       "1  1970-01-05  93.000000  94.250000  92.529999   93.459999   11490000.0   \n",
       "2  1970-01-06  93.459999  93.809998  92.129997   92.820000   11460000.0   \n",
       "3  1970-01-07  92.820000  93.379997  91.930000   92.629997   10010000.0   \n",
       "4  1970-01-08  92.629997  93.470001  91.989998   92.680000   10670000.0   \n",
       "\n",
       "   GSPC.Adjusted  Daily_average_price    T_index  MACD  ATR  ADX     PSAR  \\\n",
       "0      93.000000            92.776667 -11.200394   NaN  NaN  NaN      NaN   \n",
       "1      93.459999            93.413333 -11.686674   NaN  NaN  NaN  94.2500   \n",
       "2      92.820000            92.919998 -11.008798   NaN  NaN  NaN  94.2500   \n",
       "3      92.629997            92.646665 -10.805747   NaN  NaN  NaN  94.2500   \n",
       "4      92.680000            92.713333 -10.859265   NaN  NaN  NaN  94.1108   \n",
       "\n",
       "   EMV    runMean  Volat  DELT  \n",
       "0  NaN  92.393999    NaN   NaN  \n",
       "1  NaN  92.185999    NaN   NaN  \n",
       "2  NaN  91.805000    NaN   NaN  \n",
       "3  NaN  91.506000    NaN   NaN  \n",
       "4  NaN  91.238000    NaN   NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:21.081111Z",
     "start_time": "2022-01-28T19:23:21.065131Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:21.273151Z",
     "start_time": "2022-01-28T19:23:21.161104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>GSPC.Open</th>\n",
       "      <th>GSPC.High</th>\n",
       "      <th>GSPC.Low</th>\n",
       "      <th>GSPC.Close</th>\n",
       "      <th>GSPC.Volume</th>\n",
       "      <th>GSPC.Adjusted</th>\n",
       "      <th>Daily_average_price</th>\n",
       "      <th>T_index</th>\n",
       "      <th>MACD</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ADX</th>\n",
       "      <th>PSAR</th>\n",
       "      <th>EMV</th>\n",
       "      <th>runMean</th>\n",
       "      <th>Volat</th>\n",
       "      <th>DELT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-02-06</td>\n",
       "      <td>85.900002</td>\n",
       "      <td>86.879997</td>\n",
       "      <td>85.230003</td>\n",
       "      <td>86.330002</td>\n",
       "      <td>10150000.0</td>\n",
       "      <td>86.330002</td>\n",
       "      <td>86.146667</td>\n",
       "      <td>-4.028883</td>\n",
       "      <td>-2.398583</td>\n",
       "      <td>1.867364</td>\n",
       "      <td>65.075354</td>\n",
       "      <td>84.731522</td>\n",
       "      <td>-5.850815</td>\n",
       "      <td>86.769002</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>-0.034016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-02-09</td>\n",
       "      <td>86.330002</td>\n",
       "      <td>87.849998</td>\n",
       "      <td>86.160004</td>\n",
       "      <td>87.010002</td>\n",
       "      <td>10830000.0</td>\n",
       "      <td>87.010002</td>\n",
       "      <td>87.006668</td>\n",
       "      <td>-4.689271</td>\n",
       "      <td>-2.231210</td>\n",
       "      <td>1.848402</td>\n",
       "      <td>59.406594</td>\n",
       "      <td>84.848662</td>\n",
       "      <td>-2.354397</td>\n",
       "      <td>86.939001</td>\n",
       "      <td>0.015286</td>\n",
       "      <td>-0.013156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-02-10</td>\n",
       "      <td>87.010002</td>\n",
       "      <td>87.400002</td>\n",
       "      <td>85.580002</td>\n",
       "      <td>86.099998</td>\n",
       "      <td>10110000.0</td>\n",
       "      <td>86.099998</td>\n",
       "      <td>86.360001</td>\n",
       "      <td>-3.785442</td>\n",
       "      <td>-2.147242</td>\n",
       "      <td>1.845386</td>\n",
       "      <td>55.549605</td>\n",
       "      <td>85.028742</td>\n",
       "      <td>-2.221681</td>\n",
       "      <td>87.037001</td>\n",
       "      <td>0.015362</td>\n",
       "      <td>-0.017348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-02-11</td>\n",
       "      <td>86.099998</td>\n",
       "      <td>87.379997</td>\n",
       "      <td>85.300003</td>\n",
       "      <td>86.940002</td>\n",
       "      <td>12260000.0</td>\n",
       "      <td>86.940002</td>\n",
       "      <td>86.540001</td>\n",
       "      <td>-4.646347</td>\n",
       "      <td>-1.989977</td>\n",
       "      <td>1.870143</td>\n",
       "      <td>52.665623</td>\n",
       "      <td>85.198017</td>\n",
       "      <td>-1.808237</td>\n",
       "      <td>87.362001</td>\n",
       "      <td>0.015608</td>\n",
       "      <td>0.001728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-02-12</td>\n",
       "      <td>86.940002</td>\n",
       "      <td>87.540001</td>\n",
       "      <td>85.930000</td>\n",
       "      <td>86.730003</td>\n",
       "      <td>10010000.0</td>\n",
       "      <td>86.730003</td>\n",
       "      <td>86.733335</td>\n",
       "      <td>-4.517162</td>\n",
       "      <td>-1.860838</td>\n",
       "      <td>1.842843</td>\n",
       "      <td>49.659310</td>\n",
       "      <td>85.300003</td>\n",
       "      <td>1.005585</td>\n",
       "      <td>87.558001</td>\n",
       "      <td>0.015427</td>\n",
       "      <td>0.012137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Index   GSPC.Open  GSPC.High   GSPC.Low  GSPC.Close  GSPC.Volume  \\\n",
       "0  1970-02-06  85.900002  86.879997  85.230003   86.330002   10150000.0   \n",
       "1  1970-02-09  86.330002  87.849998  86.160004   87.010002   10830000.0   \n",
       "2  1970-02-10  87.010002  87.400002  85.580002   86.099998   10110000.0   \n",
       "3  1970-02-11  86.099998  87.379997  85.300003   86.940002   12260000.0   \n",
       "4  1970-02-12  86.940002  87.540001  85.930000   86.730003   10010000.0   \n",
       "\n",
       "   GSPC.Adjusted  Daily_average_price   T_index      MACD       ATR  \\\n",
       "0      86.330002            86.146667 -4.028883 -2.398583  1.867364   \n",
       "1      87.010002            87.006668 -4.689271 -2.231210  1.848402   \n",
       "2      86.099998            86.360001 -3.785442 -2.147242  1.845386   \n",
       "3      86.940002            86.540001 -4.646347 -1.989977  1.870143   \n",
       "4      86.730003            86.733335 -4.517162 -1.860838  1.842843   \n",
       "\n",
       "         ADX       PSAR       EMV    runMean     Volat      DELT  \n",
       "0  65.075354  84.731522 -5.850815  86.769002  0.015200 -0.034016  \n",
       "1  59.406594  84.848662 -2.354397  86.939001  0.015286 -0.013156  \n",
       "2  55.549605  85.028742 -2.221681  87.037001  0.015362 -0.017348  \n",
       "3  52.665623  85.198017 -1.808237  87.362001  0.015608  0.001728  \n",
       "4  49.659310  85.300003  1.005585  87.558001  0.015427  0.012137  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:23:23.726325Z",
     "start_time": "2022-01-28T19:23:23.686329Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data[['MACD', 'ATR', 'ADX', 'EMV', 'PSAR', 'Volat', 'DELT', 'runMean']]\n",
    "y = data['T_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split to Train and Test with Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:12:52.377384Z",
     "start_time": "2022-01-27T18:12:41.102850Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from math import *\n",
    "\n",
    "sliding_window = 50\n",
    "\n",
    "#splitting data into training (75%) and test(25%) sets\n",
    "training_dataset_length = math.ceil(len(X) * .75)\n",
    "\n",
    "sc = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train_data = X.iloc[0:training_dataset_length, :]\n",
    "SC = sc.fit(train_data)\n",
    "train_data = SC.transform(train_data)\n",
    "test_data = X.iloc[training_dataset_length-sliding_window:, :]\n",
    "test_data = SC.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:12:52.569279Z",
     "start_time": "2022-01-27T18:12:52.492319Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_train will contain values of sliding windows\n",
    "# y_train will contain values of every sliding window+1 values which we want to predict\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(sliding_window, training_dataset_length):\n",
    "    x_train.append(train_data[i-sliding_window:i, :])\n",
    "    y_train.append(y[i])\n",
    "\n",
    "    \n",
    "# y_test will contain values of every sliding window+1 values which we want to predict\n",
    "y_test = y[training_dataset_length:]\n",
    "\n",
    "# x_test will contain values of sliding windows\n",
    "x_test = []\n",
    "for i in range(sliding_window, len(test_data)):\n",
    "    x_test.append(test_data[i-sliding_window:i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:14:00.596983Z",
     "start_time": "2022-01-27T18:14:00.550054Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:14:00.704481Z",
     "start_time": "2022-01-27T18:14:00.688856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8648, 50, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Tools\n",
    "\n",
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:14:17.452782Z",
     "start_time": "2022-01-27T18:14:17.432792Z"
    }
   },
   "outputs": [],
   "source": [
    "# flatten input\n",
    "n_input = x_train.shape[1] * x_train.shape[2]\n",
    "x_train_2D = x_train.reshape((x_train.shape[0], n_input))\n",
    "n_input = x_test.shape[1] * x_test.shape[2]\n",
    "x_test_2D = x_test.reshape((x_test.shape[0], n_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:15:18.529776Z",
     "start_time": "2022-01-27T18:14:25.059918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "271/271 [==============================] - 2s 2ms/step - loss: 0.0191\n",
      "Epoch 2/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 3/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 4/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 5/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 6/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0128\n",
      "Epoch 7/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 8/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0127\n",
      "Epoch 9/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0128\n",
      "Epoch 10/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0122\n",
      "Epoch 11/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0126\n",
      "Epoch 12/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0119\n",
      "Epoch 13/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 14/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0121\n",
      "Epoch 15/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0120\n",
      "Epoch 16/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "Epoch 17/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0120\n",
      "Epoch 18/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 19/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 20/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 21/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 22/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0120\n",
      "Epoch 23/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 24/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 25/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 26/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 27/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0117\n",
      "Epoch 28/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0117- ETA: 0s - loss: 0\n",
      "Epoch 29/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 30/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 31/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 32/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 33/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 34/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0116\n",
      "Epoch 35/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 36/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 37/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0116\n",
      "Epoch 38/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 39/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 40/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 41/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 42/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118- ETA: 0s - loss: 0.0\n",
      "Epoch 43/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 44/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 45/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 46/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 47/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 48/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 49/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 50/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 51/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 52/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 53/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 54/100\n",
      "271/271 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 55/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 56/100\n",
      "271/271 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 57/100\n",
      "271/271 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 58/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 59/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 60/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 61/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 62/100\n",
      "271/271 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 63/100\n",
      "271/271 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 64/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 65/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 66/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 67/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 68/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 69/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 70/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 71/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 72/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 73/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 74/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 75/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 76/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 77/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 78/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 79/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 80/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 81/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 82/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 83/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 84/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 85/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 86/100\n",
      "271/271 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 87/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 88/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 89/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 90/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 91/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 92/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 93/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 94/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 95/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 96/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 97/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 98/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 99/100\n",
      "271/271 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 100/100\n",
      "271/271 [==============================] - 0s 2ms/step - loss: 0.0118\n"
     ]
    }
   ],
   "source": [
    "# Initialising the MLP\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "MLPmodel = Sequential()\n",
    "MLPmodel.add(Dense(100, activation='relu', input_dim=x_train_2D.shape[1]))\n",
    "MLPmodel.add(Dense(1))\n",
    "MLPmodel.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "MLPmodel.fit(x_train_2D, y_train, epochs=100, verbose=1)\n",
    "\n",
    "# check predicted values\n",
    "predictions = MLPmodel.predict(x_test_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T17:30:09.306646Z",
     "start_time": "2022-01-27T17:30:08.380492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_RMSE: 0.011273219977976747\n"
     ]
    }
   ],
   "source": [
    "error = np.sqrt(mean_squared_error(y_test, predictions, multioutput='uniform_average'))\n",
    "print('MLP_RMSE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T15:58:15.993090Z",
     "start_time": "2022-01-27T15:49:55.273811Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18/18 [==============================] - 108s 5s/step - loss: 0.0122\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 90s 5s/step - loss: 0.0118\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 97s 5s/step - loss: 0.0118\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.0118\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 97s 5s/step - loss: 0.0118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc600e04c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import LSTM, Dropout\n",
    "\n",
    "# Initialising the LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=200, return_sequences=True, activation='relu',input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and Dropout layer\n",
    "model.add(LSTM(units=200,activation='relu', return_sequences=True))\n",
    "\n",
    "# Adding a third LSTM layer and Dropout layer\n",
    "model.add(LSTM(units=200, return_sequences=True,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and and Dropout layer\n",
    "model.add(LSTM(units=200))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "# For Full connection layer we use dense\n",
    "# As the output is 1D so we use unit=1\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# compile and fit the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T15:58:19.788313Z",
     "start_time": "2022-01-27T15:58:19.453816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_RMSE: 0.02871761071832099\n"
     ]
    }
   ],
   "source": [
    "error = np.sqrt(mean_squared_error(y_test, predictions, multioutput='uniform_average'))\n",
    "print('LSTM_RMSE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T16:02:10.703694Z",
     "start_time": "2022-01-27T16:01:02.909551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "271/271 [==============================] - 4s 5ms/step - loss: 0.0142\n",
      "Epoch 2/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0123\n",
      "Epoch 3/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0118\n",
      "Epoch 4/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0116\n",
      "Epoch 5/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0114\n",
      "Epoch 6/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0115\n",
      "Epoch 7/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0114\n",
      "Epoch 8/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0113\n",
      "Epoch 9/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0111\n",
      "Epoch 10/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0110\n",
      "Epoch 11/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0108\n",
      "Epoch 12/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0105\n",
      "Epoch 13/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0107\n",
      "Epoch 14/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0101\n",
      "Epoch 15/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0100\n",
      "Epoch 16/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0095\n",
      "Epoch 17/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0090\n",
      "Epoch 18/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0082\n",
      "Epoch 19/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0070\n",
      "Epoch 20/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0062\n",
      "Epoch 21/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0061\n",
      "Epoch 22/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0054\n",
      "Epoch 23/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0044\n",
      "Epoch 24/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0035\n",
      "Epoch 25/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0033\n",
      "Epoch 26/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0032\n",
      "Epoch 27/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0023\n",
      "Epoch 28/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0017\n",
      "Epoch 29/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.0017\n",
      "Epoch 30/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0023\n",
      "Epoch 31/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0011\n",
      "Epoch 32/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0011\n",
      "Epoch 33/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 7.1952e-04\n",
      "Epoch 34/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0021\n",
      "Epoch 35/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0012\n",
      "Epoch 36/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 7.9585e-04\n",
      "Epoch 37/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 7.9777e-04\n",
      "Epoch 38/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 7.0871e-04\n",
      "Epoch 39/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0011\n",
      "Epoch 40/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 7.5706e-04\n",
      "Epoch 41/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 4.5996e-04\n",
      "Epoch 42/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.0013\n",
      "Epoch 43/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 6.2500e-04\n",
      "Epoch 44/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 6.9561e-04\n",
      "Epoch 45/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 9.4623e-04\n",
      "Epoch 46/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 6.0100e-04\n",
      "Epoch 47/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 4.9555e-04\n",
      "Epoch 48/50\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 7.7888e-04\n",
      "Epoch 49/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 4.9864e-04\n",
      "Epoch 50/50\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 7.7504e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc02fa0a60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "modelCNN = Sequential()\n",
    "modelCNN.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "modelCNN.add(MaxPooling1D(pool_size=2))\n",
    "modelCNN.add(Flatten())\n",
    "modelCNN.add(Dense(50, activation='relu'))\n",
    "modelCNN.add(Dense(1))\n",
    "modelCNN.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "modelCNN.fit(x_train, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T16:02:11.883691Z",
     "start_time": "2022-01-27T16:02:10.912087Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_RMSE: 0.015608098290853323\n"
     ]
    }
   ],
   "source": [
    "predictions = modelCNN.predict(x_test)\n",
    "\n",
    "error = np.sqrt(mean_squared_error(y_test, predictions, multioutput='uniform_average'))\n",
    "print('CNN_RMSE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:53:06.089508Z",
     "start_time": "2022-01-27T18:53:06.065505Z"
    }
   },
   "outputs": [],
   "source": [
    "import tslearn\n",
    "from tslearn import svm \n",
    "\n",
    "svr = svm.TimeSeriesSVR(kernel=\"poly\", verbose=1, max_iter=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:53:07.667811Z",
     "start_time": "2022-01-27T18:53:07.624590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TimeSeriesSVR in module tslearn.svm.svm:\n",
      "\n",
      "class TimeSeriesSVR(TimeSeriesSVMMixin, sklearn.base.RegressorMixin, tslearn.bases.bases.TimeSeriesBaseEstimator)\n",
      " |  TimeSeriesSVR(C=1.0, kernel='gak', degree=3, gamma='auto', coef0=0.0, tol=0.001, epsilon=0.1, shrinking=True, cache_size=200, n_jobs=None, verbose=0, max_iter=-1)\n",
      " |  \n",
      " |  Time-series specific Support Vector Regressor.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  C : float, optional (default=1.0)\n",
      " |      Penalty parameter C of the error term.\n",
      " |  \n",
      " |  kernel : string, optional (default='gak')\n",
      " |       Specifies the kernel type to be used in the algorithm.\n",
      " |       It must be one of 'gak' or a kernel accepted by ``sklearn.svm.SVC``.\n",
      " |       If none is given, 'gak' will be used. If a callable is given it is\n",
      " |       used to pre-compute the kernel matrix from data matrices; that matrix\n",
      " |       should be an array of shape ``(n_samples, n_samples)``.\n",
      " |  \n",
      " |  degree : int, optional (default=3)\n",
      " |      Degree of the polynomial kernel function ('poly').\n",
      " |      Ignored by all other kernels.\n",
      " |  \n",
      " |  gamma : float, optional (default='auto')\n",
      " |      Kernel coefficient for 'gak', 'rbf', 'poly' and 'sigmoid'.\n",
      " |      If gamma is 'auto' then:\n",
      " |  \n",
      " |      - for 'gak' kernel, it is computed based on a sampling of the training\n",
      " |        set (cf :ref:`tslearn.metrics.gamma_soft_dtw <fun-tslearn.metrics.gamma_soft_dtw>`)\n",
      " |      - for other kernels (eg. 'rbf'), 1/n_features will be used.\n",
      " |  \n",
      " |  coef0 : float, optional (default=0.0)\n",
      " |      Independent term in kernel function.\n",
      " |      It is only significant in 'poly' and 'sigmoid'.\n",
      " |  \n",
      " |  tol : float, optional (default=1e-3)\n",
      " |      Tolerance for stopping criterion.\n",
      " |  \n",
      " |  epsilon : float, optional (default=0.1)\n",
      " |       Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n",
      " |       within which no penalty is associated in the training loss function\n",
      " |       with points predicted within a distance epsilon from the actual\n",
      " |       value.\n",
      " |  \n",
      " |  shrinking : boolean, optional (default=True)\n",
      " |      Whether to use the shrinking heuristic.\n",
      " |  \n",
      " |  cache_size :  float, optional (default=200.0)\n",
      " |      Specify the size of the kernel cache (in MB).\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      The number of jobs to run in parallel for GAK cross-similarity matrix\n",
      " |      computations.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See scikit-learns'\n",
      " |      `Glossary <https://scikit-learn.org/stable/glossary.html#term-n-jobs>`_\n",
      " |      for more details.\n",
      " |  \n",
      " |  verbose : int, default: 0\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  max_iter : int, optional (default=-1)\n",
      " |      Hard limit on iterations within solver, or -1 for no limit.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  support_ : array-like, shape = [n_SV]\n",
      " |      Indices of support vectors.\n",
      " |      \n",
      " |  support_vectors_ : array of shape [n_SV, sz, d]\n",
      " |      Support vectors in tslearn dataset format\n",
      " |  \n",
      " |  dual_coef_ : array, shape = [1, n_SV]\n",
      " |      Coefficients of the support vector in the decision function.\n",
      " |  \n",
      " |  coef_ : array, shape = [1, n_features]\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem). This is only available in the case of a linear kernel.\n",
      " |      `coef_` is readonly property derived from `dual_coef_` and\n",
      " |      `support_vectors_`.\n",
      " |  \n",
      " |  intercept_ : array, shape = [1]\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  sample_weight : array-like, shape = [n_samples]\n",
      " |      Individual weights for each sample\n",
      " |  \n",
      " |  svm_estimator_ : sklearn.svm.SVR\n",
      " |      The underlying sklearn estimator\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from tslearn.generators import random_walk_blobs\n",
      " |  >>> X, y = random_walk_blobs(n_ts_per_blob=10, sz=64, d=2, n_blobs=2)\n",
      " |  >>> import numpy\n",
      " |  >>> y = y.astype(numpy.float) + numpy.random.randn(20) * .1\n",
      " |  >>> reg = TimeSeriesSVR(kernel=\"gak\", gamma=\"auto\")\n",
      " |  >>> reg.fit(X, y).predict(X).shape\n",
      " |  (20,)\n",
      " |  >>> sv = reg.support_vectors_\n",
      " |  >>> sv.shape  # doctest: +ELLIPSIS\n",
      " |  (..., 64, 2)\n",
      " |  >>> sv.shape[0] <= 20\n",
      " |  True\n",
      " |  \n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Fast Global Alignment Kernels.\n",
      " |  Marco Cuturi.\n",
      " |  ICML 2011.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TimeSeriesSVR\n",
      " |      TimeSeriesSVMMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      tslearn.bases.bases.TimeSeriesBaseEstimator\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, C=1.0, kernel='gak', degree=3, gamma='auto', coef0=0.0, tol=0.001, epsilon=0.1, shrinking=True, cache_size=200, n_jobs=None, verbose=0, max_iter=-1)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the SVM model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape=(n_ts, sz, d)\n",
      " |          Time series dataset.\n",
      " |          \n",
      " |      y : array-like of shape=(n_ts, )\n",
      " |          Time series labels.\n",
      " |          \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Per-sample weights. Rescale C per sample. Higher weights force the \n",
      " |          classifier to put more emphasis on these points.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for a given set of time series.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape=(n_ts, sz, d)\n",
      " |          Time series dataset.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array of shape=(n_ts, ) or (n_ts, dim_output), depending on the shape\n",
      " |      of the target vector provided at training time.\n",
      " |          Predicted targets\n",
      " |  \n",
      " |  support_vectors_time_series_(self, X=None)\n",
      " |      DEPRECATED: The use of `support_vectors_time_series_` is deprecated in tslearn v0.4 and will be removed in v0.6. Use `support_vectors_` property instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  n_iter_\n",
      " |  \n",
      " |  support_vectors_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from TimeSeriesSVMMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(svm.TimeSeriesSVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:53:10.475261Z",
     "start_time": "2022-01-27T18:53:08.267262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasia\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    }
   ],
   "source": [
    "svr.fit(x_train, y_train)\n",
    "predictions = svr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:53:59.573677Z",
     "start_time": "2022-01-27T18:53:59.553686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR_RMSE: 4.063749122652865\n"
     ]
    }
   ],
   "source": [
    "error = np.sqrt(mean_squared_error(y_test, predictions, multioutput='uniform_average'))\n",
    "print('SVR_RMSE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:21:53.081952Z",
     "start_time": "2022-01-27T18:21:53.050950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Earth in module pyearth.earth:\n",
      "\n",
      "class Earth(_Earth, sklearn.base.RegressorMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  Earth(max_terms=None, max_degree=None, allow_missing=False, penalty=None, endspan_alpha=None, endspan=None, minspan_alpha=None, minspan=None, thresh=None, zero_tol=None, min_search_points=None, check_every=None, allow_linear=None, use_fast=None, fast_K=None, fast_h=None, smooth=None, enable_pruning=True, feature_importance_type=None, verbose=0)\n",
      " |  \n",
      " |  Multivariate Adaptive Regression Splines\n",
      " |  \n",
      " |  A flexible regression method that automatically searches for interactions\n",
      " |  and non-linear relationships.  Earth models can be thought of as\n",
      " |  linear models in a higher dimensional basis space\n",
      " |  (specifically, a multivariate truncated power spline basis).\n",
      " |  Each term in an Earth model is a product of so called \"hinge functions\".\n",
      " |  A hinge function is a function that's equal to its argument where that\n",
      " |  argument is greater than zero and is zero everywhere else.\n",
      " |  \n",
      " |  The multivariate adaptive regression splines algorithm has two stages.\n",
      " |  First, the forward pass searches for terms in the truncated power spline\n",
      " |  basis that locally minimize the squared error loss of the training set.\n",
      " |  Next, a pruning pass selects a subset of those terms that produces\n",
      " |  a locally minimal generalized cross-validation (GCV) score.  The GCV score\n",
      " |  is not actually based on cross-validation, but rather is meant to\n",
      " |  approximate a true cross-validation score by penalizing model complexity.\n",
      " |  The final result is a set of terms that is nonlinear in the original\n",
      " |  feature space, may include interactions, and is likely to generalize well.\n",
      " |  \n",
      " |  The Earth class supports dense input only.  Data structures from the\n",
      " |  pandas and patsy modules are supported, but are copied into numpy arrays\n",
      " |  for computation.  No copy is made if the inputs are numpy float64 arrays.\n",
      " |  Earth objects can be serialized using the pickle module and copied\n",
      " |  using the copy module.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  max_terms : int, optional (default=min(2 * n + m // 10, 400)), \n",
      " |                             where n is the number of features and m is the number \n",
      " |                             of rows)\n",
      " |      The maximum number of terms generated by the forward pass.  All memory is\n",
      " |      allocated at the beginning of the forward pass, so setting max_terms to \n",
      " |      a very high number on a system with insufficient memory may cause a \n",
      " |      MemoryError at the start of the forward pass.\n",
      " |  \n",
      " |  \n",
      " |  max_degree : int, optional (default=1)\n",
      " |      The maximum degree of terms generated by the forward pass.\n",
      " |  \n",
      " |  \n",
      " |  allow_missing : boolean, optional (default=False)\n",
      " |      If True, use missing data method described in [3].\n",
      " |      Use missing argument to determine missingness or,if X is a pandas\n",
      " |      DataFrame, infer missingness from X.\n",
      " |  \n",
      " |  \n",
      " |  penalty : float, optional (default=3.0)\n",
      " |      A smoothing parameter used to calculate GCV and GRSQ.\n",
      " |      Used during the pruning pass and to determine whether to add a hinge\n",
      " |      or linear basis function during the forward pass.\n",
      " |      See the d parameter in equation 32, Friedman, 1991.\n",
      " |  \n",
      " |  \n",
      " |  endspan_alpha : float, optional, probability between 0 and 1 (default=0.05)\n",
      " |      A parameter controlling the calculation of the endspan\n",
      " |      parameter (below).  The endspan parameter is calculated as\n",
      " |      round(3 - log2(endspan_alpha/n)), where n is the number of features.\n",
      " |      The endspan_alpha parameter represents the probability of a run of\n",
      " |      positive or negative error values on either end of the data vector\n",
      " |      of any feature in the data set.  See equation 45, Friedman, 1991.\n",
      " |  \n",
      " |  \n",
      " |  endspan : int, optional (default=-1)\n",
      " |      The number of extreme data values of each feature not eligible\n",
      " |      as knot locations. If endspan is set to -1 (default) then the\n",
      " |      endspan parameter is calculated based on endspan_alpah (above).\n",
      " |      If endspan is set to a positive integer then endspan_alpha is ignored.\n",
      " |  \n",
      " |  \n",
      " |  minspan_alpha : float, optional, probability between 0 and 1 (default=0.05)\n",
      " |      A parameter controlling the calculation of the minspan\n",
      " |      parameter (below).  The minspan parameter is calculated as\n",
      " |  \n",
      " |          (int) -log2(-(1.0/(n*count))*log(1.0-minspan_alpha)) / 2.5\n",
      " |  \n",
      " |      where n is the number of features and count is the number of points at\n",
      " |      which the parent term is non-zero.  The minspan_alpha parameter\n",
      " |      represents the probability of a run of positive or negative error\n",
      " |      values between adjacent knots separated by minspan intervening\n",
      " |      data points. See equation 43, Friedman, 1991.\n",
      " |  \n",
      " |  \n",
      " |  minspan : int, optional (default=-1)\n",
      " |      The minimal number of data points between knots.  If minspan is set\n",
      " |      to -1 (default) then the minspan parameter is calculated based on\n",
      " |      minspan_alpha (above).  If minspan is set to a positive integer then\n",
      " |      minspan_alpha is ignored.\n",
      " |  \n",
      " |  \n",
      " |  thresh : float, optional (default=0.001)\n",
      " |      Parameter used when evaluating stopping conditions for the forward\n",
      " |      pass. If either RSQ > 1 - thresh or if RSQ increases by less than\n",
      " |      thresh for a forward pass iteration then the forward pass is\n",
      " |      terminated.\n",
      " |  \n",
      " |  \n",
      " |  zero_tol : float, optional (default=1e-12)\n",
      " |      Used when determining whether a floating point number is zero during\n",
      " |      the  forward pass.  This is important in determining linear dependence\n",
      " |      and in the fast update procedure.  There should normally be no reason\n",
      " |      to change  zero_tol from its default. However, if nans are showing up\n",
      " |      during the forward pass or the forward pass seems to be terminating\n",
      " |      unexpectedly, consider adjusting zero_tol.\n",
      " |  \n",
      " |  min_search_points : int, optional (default=100)\n",
      " |      Used to calculate check_every (below).  The minimum samples necessary\n",
      " |      for check_every to be greater than 1.  The check_every parameter\n",
      " |      is calculated as\n",
      " |  \n",
      " |           (int) m / min_search_points\n",
      " |  \n",
      " |      if m > min_search_points, where m is the number of samples in the\n",
      " |      training set.  If m <= min_search_points then check_every is set to 1.\n",
      " |  \n",
      " |  \n",
      " |  check_every : int, optional (default=-1)\n",
      " |      If check_every > 0, only one of every check_every sorted data points\n",
      " |      is considered as a candidate knot.  If check_every is set to -1 then\n",
      " |      the check_every parameter is calculated based on\n",
      " |      min_search_points (above).\n",
      " |  \n",
      " |  \n",
      " |  allow_linear : bool, optional (default=True)\n",
      " |      If True, the forward pass will check the GCV of each new pair of terms\n",
      " |      and, if it's not an improvement on a single term with no knot (called a\n",
      " |      linear term, although it may actually be a product of a linear term\n",
      " |      with some other parent term), then only that single, knotless term will\n",
      " |      be used. If False, that behavior is disabled and all terms will have\n",
      " |      knots except those with variables specified by the linvars argument\n",
      " |      (see the fit method).\n",
      " |  \n",
      " |  use_fast : bool, optional (default=False)\n",
      " |      if True, use the approximation procedure defined in [2] to speed up the\n",
      " |      forward pass. The procedure uses two hyper-parameters : fast_K\n",
      " |      and fast_h. Check below for more details.\n",
      " |  \n",
      " |  fast_K : int, optional (default=5)\n",
      " |      Only used if use_fast is True. As defined in [2], section 3.0, it\n",
      " |      defines the maximum number of basis functions to look at when\n",
      " |      we search for a parent, that is we look at only the fast_K top\n",
      " |      terms ranked by the mean squared error of the model the last time\n",
      " |      the term was chosen as a parent. The smaller fast_K is, the more\n",
      " |      gains in speed we get but the more approximate is the result.\n",
      " |      If fast_K is the maximum number of terms and fast_h is 1,\n",
      " |      the behavior is the same as in the normal case\n",
      " |      (when use_fast is False).\n",
      " |  \n",
      " |  fast_h : int, optional (default=1)\n",
      " |      Only used if use_fast is True. As defined in [2], section 4.0, it\n",
      " |      determines the number of iterations before repassing through all\n",
      " |      the variables when searching for the variable to use for a\n",
      " |      given parent term. Before reaching fast_h number of iterations\n",
      " |      only the last chosen variable for the parent term is used. The\n",
      " |      bigger fast_h is, the more speed gains we get, but the result\n",
      " |      is more approximate.\n",
      " |  \n",
      " |  smooth : bool, optional (default=False)\n",
      " |      If True, the model will be smoothed such that it has continuous first\n",
      " |      derivatives.\n",
      " |      For details, see section 3.7, Friedman, 1991.\n",
      " |  \n",
      " |  enable_pruning : bool, optional(default=True)\n",
      " |      If False, the pruning pass will be skipped.\n",
      " |  \n",
      " |  feature_importance_type: string or list of strings, optional (default=None)\n",
      " |      Specify which kind of feature importance criteria to compute.\n",
      " |      Currently three criteria are supported : 'gcv', 'rss' and 'nb_subsets'.\n",
      " |      By default (when it is None), no feature importance is computed.\n",
      " |      Feature importance is a measure of the effect of the features\n",
      " |      on the outputs. For each feature, the values go from\n",
      " |      0 to 1 and sum up to 1. A high value means the feature have in average\n",
      " |      (over the population) a large effect on the outputs.\n",
      " |      See [4], section 12.3 for more information about the criteria.\n",
      " |  \n",
      " |  verbose : int, optional(default=0)\n",
      " |      If verbose >= 1, print out progress information during fitting.  If\n",
      " |      verbose >= 2, also print out information on numerical difficulties\n",
      " |      if encountered during fitting. If verbose >= 3, print even more\n",
      " |      information that is probably only useful to the developers of py-earth.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  `coef_` : array, shape = [pruned basis length, number of outputs]\n",
      " |      The weights of the model terms that have not been pruned.\n",
      " |  \n",
      " |  \n",
      " |  `basis_` : _basis.Basis\n",
      " |      An object representing model terms.  Each term is a product of\n",
      " |      constant, linear, and hinge functions of the input features.\n",
      " |  \n",
      " |  \n",
      " |  `mse_` : float\n",
      " |      The mean squared error of the model after the final linear fit.\n",
      " |      If sample_weight and/or output_weight are given, this score is\n",
      " |      weighted appropriately.\n",
      " |  \n",
      " |  \n",
      " |  `rsq_` : float\n",
      " |      The generalized r^2 of the model after the final linear fit.\n",
      " |      If sample_weight and/or output_weight are given, this score is\n",
      " |      weighted appropriately.\n",
      " |  \n",
      " |  `gcv_` : float\n",
      " |      The generalized cross validation (GCV) score of the model after the\n",
      " |      final linear fit. If sample_weight and/or output_weight are\n",
      " |      given, this score is weighted appropriately.\n",
      " |  \n",
      " |  `grsq_` : float\n",
      " |      An r^2 like score based on the GCV. If sample_weight and/or\n",
      " |      output_weight are given, this score is\n",
      " |      weighted appropriately.\n",
      " |  \n",
      " |  `forward_pass_record_` : _record.ForwardPassRecord\n",
      " |      An object containing information about the forward pass, such as\n",
      " |      training loss function values after each iteration and the final\n",
      " |      stopping condition.\n",
      " |  \n",
      " |  \n",
      " |  `pruning_pass_record_` : _record.PruningPassRecord\n",
      " |      An object containing information about the pruning pass, such as\n",
      " |      training loss function values after each iteration and the\n",
      " |      selected optimal iteration.\n",
      " |  \n",
      " |  \n",
      " |  `xlabels_` : list\n",
      " |      List of column names for training predictors.\n",
      " |      Defaults to ['x0','x1',....] if column names are not provided.\n",
      " |  \n",
      " |  \n",
      " |  `allow_missing_` : list\n",
      " |      List of booleans indicating whether each variable is allowed to\n",
      " |      be missing.  Set during training.  A variable may be missing\n",
      " |      only if fitting included missing data for that variable.\n",
      " |  \n",
      " |  `feature_importances_`: array of shape [m] or dict\n",
      " |      m is the number of features.\n",
      " |      if one feature importance type is specified, it is an\n",
      " |      array of shape m. If several feature importance types are\n",
      " |      specified, then it is dict where each key is a feature importance type\n",
      " |      name and its corresponding value is an array of shape m.\n",
      " |  \n",
      " |  `_version`: string\n",
      " |      The version of py-earth in which the Earth object was originally \n",
      " |      created.  This information may be useful when dealing with \n",
      " |      serialized Earth objects.\n",
      " |  \n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] Friedman, Jerome. Multivariate Adaptive Regression Splines.\n",
      " |         Annals of Statistics. Volume 19, Number 1 (1991), 1-67.\n",
      " |  \n",
      " |  .. [2] Fast MARS, Jerome H.Friedman, Technical Report No.110, May 1993.\n",
      " |  \n",
      " |  .. [3] Estimating Functions of Mixed Ordinal and Categorical Variables\n",
      " |         Using Adaptive Splines, Jerome H.Friedman, Technical Report\n",
      " |         No.108, June 1991.\n",
      " |  \n",
      " |  .. [4] http://www.milbo.org/doc/earth-notes.pdf\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Earth\n",
      " |      _Earth\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, max_terms=None, max_degree=None, allow_missing=False, penalty=None, endspan_alpha=None, endspan=None, minspan_alpha=None, minspan=None, thresh=None, zero_tol=None, min_search_points=None, check_every=None, allow_linear=None, use_fast=None, fast_K=None, fast_h=None, smooth=None, enable_pruning=True, feature_importance_type=None, verbose=0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None, output_weight=None, missing=None, xlabels=None, linvars=[])\n",
      " |      Fit an Earth model to the input data X and y.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features the training predictors.\n",
      " |          The X parameter can be a numpy array, a pandas DataFrame, a patsy\n",
      " |          DesignMatrix, or a tuple of patsy DesignMatrix objects as\n",
      " |          output by patsy.dmatrices.\n",
      " |      \n",
      " |      \n",
      " |      y : array-like, optional (default=None), shape = [m, p] where m is the\n",
      " |          number of samples The training response, p the number of outputs.\n",
      " |          The y parameter can be a numpy array, a pandas DataFrame,\n",
      " |          a Patsy DesignMatrix, or can be left as None (default) if X was\n",
      " |          the output of a call to patsy.dmatrices (in which case, X contains\n",
      " |          the response).\n",
      " |      \n",
      " |      \n",
      " |      sample_weight : array-like, optional (default=None), shape = [m]\n",
      " |           where m is the number of samples.\n",
      " |           Sample weights for training.  Weights must be greater than or\n",
      " |           equal to zero. Rows with zero weight do not contribute at all.\n",
      " |           Weights are useful when dealing with heteroscedasticity.\n",
      " |           In such cases, the weight should be proportional to the inverse of\n",
      " |           the (known) variance.\n",
      " |      \n",
      " |      output_weight : array-like, optional (default=None), shape = [p]\n",
      " |           where p is the number of outputs.\n",
      " |           Output weights for training. Weights must be greater than or equal\n",
      " |           to zero. Output with zero weight do not contribute at all.\n",
      " |      \n",
      " |      missing : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features.\n",
      " |          The missing parameter can be a numpy array, a pandas DataFrame, or\n",
      " |          a  patsy DesignMatrix.  All entries will be interpreted as boolean\n",
      " |          values, with True indicating the corresponding entry in X should be\n",
      " |          interpreted as missing.  If the missing argument not used but the X\n",
      " |          argument is a pandas DataFrame, missing will be inferred from X if\n",
      " |          allow_missing is True.\n",
      " |      \n",
      " |      linvars : iterable of strings or ints, optional (empty by default)\n",
      " |          Used to specify features that may only enter terms as linear basis\n",
      " |          functions (without knots).  Can include both column numbers and\n",
      " |          column names (see xlabels, below).  If left empty, some variables\n",
      " |          may still enter linearly during the forward pass if no knot would\n",
      " |          provide a reduction in GCV compared to the linear function.\n",
      " |          Note that this feature differs from the R package earth.\n",
      " |      \n",
      " |      \n",
      " |      xlabels : iterable of strings, optional (empty by default)\n",
      " |          The xlabels argument can be used to assign names to data columns.\n",
      " |          This argument is not generally needed, as names can be captured\n",
      " |          automatically from most standard data structures.\n",
      " |          If included, must have length n, where n is the number of features.\n",
      " |          Note that column order is used to compute term values and make\n",
      " |          predictions, not column names.\n",
      " |  \n",
      " |  forward_pass(self, X, y=None, sample_weight=None, output_weight=None, missing=None, xlabels=None, linvars=[], skip_scrub=False)\n",
      " |      Perform the forward pass of the multivariate adaptive regression\n",
      " |      splines algorithm.  Users will normally want to call the fit method\n",
      " |      instead, which performs the forward pass, the pruning pass,\n",
      " |      and a linear fit to determine the final model coefficients.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [m, n] where m is the number of samples and n\n",
      " |          is the number of features The training predictors.\n",
      " |          The X parameter can be a numpy array, a pandas DataFrame, a patsy\n",
      " |          DesignMatrix, or a tuple of patsy DesignMatrix objects as output\n",
      " |          by patsy.dmatrices.\n",
      " |      \n",
      " |      y : array-like, optional (default=None), shape = [m, p] where m is the\n",
      " |          number of samples, p the number of outputs.\n",
      " |          The y parameter can be a numpy array, a pandas DataFrame,\n",
      " |          a Patsy DesignMatrix, or can be left as None (default) if X was\n",
      " |          the output of a call to patsy.dmatrices (in which case, X contains\n",
      " |          the response).\n",
      " |      \n",
      " |      sample_weight : array-like, optional (default=None), shape = [m]\n",
      " |           where m is the number of samples.\n",
      " |           Sample weights for training.  Weights must be greater than or\n",
      " |           equal to zero. Rows with zero weight do not contribute at all.\n",
      " |           Weights are useful when dealing with heteroscedasticity.  In such\n",
      " |           cases, the weight should be proportional to the inverse of the\n",
      " |           (known) variance.\n",
      " |      \n",
      " |      output_weight : array-like, optional (default=None), shape = [p]\n",
      " |           where p is the number of outputs.\n",
      " |           The total mean squared error (MSE) is a weighted sum of\n",
      " |           mean squared errors (MSE) associated to each output, where\n",
      " |           the weights are given by output_weight.\n",
      " |           Output weights must be greater than or equal\n",
      " |           to zero. Outputs with zero weight do not contribute at all\n",
      " |           to the total mean squared error (MSE).\n",
      " |      \n",
      " |      missing : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features.\n",
      " |          The missing parameter can be a numpy array, a pandas DataFrame, or\n",
      " |          a  patsy DesignMatrix.  All entries will be interpreted as boolean\n",
      " |          values, with True indicating the corresponding entry in X should be\n",
      " |          interpreted as missing.  If the missing argument not used but the X\n",
      " |          argument is a pandas DataFrame, missing will be inferred from X if\n",
      " |          allow_missing is True.\n",
      " |      \n",
      " |      linvars : iterable of strings or ints, optional (empty by default)\n",
      " |          Used to specify features that may only enter terms as linear basis\n",
      " |          functions (without knots).  Can include both column numbers and\n",
      " |          column names (see xlabels, below).\n",
      " |      \n",
      " |      \n",
      " |      xlabels : iterable of strings, optional (empty by default)\n",
      " |          The xlabels argument can be used to assign names to data columns.\n",
      " |          This argument is not generally needed, as names can be captured\n",
      " |          automatically from most standard data structures.\n",
      " |          If included, must have length n, where n is the number of features.\n",
      " |          Note that column order is used to compute term values and make\n",
      " |          predictions, not column names.\n",
      " |  \n",
      " |  forward_trace(self)\n",
      " |      Return information about the forward pass.\n",
      " |  \n",
      " |  get_penalty(self)\n",
      " |      Get the penalty parameter being used.  Default is 3.\n",
      " |  \n",
      " |  linear_fit(self, X, y=None, sample_weight=None, output_weight=None, missing=None, skip_scrub=False)\n",
      " |      Solve the linear least squares problem to determine the coefficients\n",
      " |      of the unpruned basis functions.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [m, n] where m is the number of samples and n\n",
      " |          is the number of features The training predictors.  The X parameter\n",
      " |          can be a numpy array, a pandas DataFrame, a patsy\n",
      " |          DesignMatrix, or a tuple of patsy DesignMatrix objects as output\n",
      " |          by patsy.dmatrices.\n",
      " |      \n",
      " |      y : array-like, optional (default=None), shape = [m, p] where m is the\n",
      " |          number of samples, p the number of outputs.\n",
      " |          The y parameter can be a numpy array, a pandas DataFrame,\n",
      " |          a Patsy DesignMatrix, or can be left as None (default) if X was\n",
      " |          the output of a call to patsy.dmatrices (in which case, X contains\n",
      " |          the response).\n",
      " |      \n",
      " |      sample_weight : array-like, optional (default=None), shape = [m]\n",
      " |           where m is the number of samples.\n",
      " |           Sample weights for training.  Weights must be greater than or\n",
      " |           equal to zero. Rows with zero weight do not contribute at all.\n",
      " |           Weights are useful when dealing with heteroscedasticity.  In such\n",
      " |           cases, the weight should be proportional to the inverse of the\n",
      " |           (known) variance.\n",
      " |      \n",
      " |      output_weight : array-like, optional (default=None), shape = [p]\n",
      " |           where p is the number of outputs.\n",
      " |           The total mean squared error (MSE) is a weighted sum of\n",
      " |           mean squared errors (MSE) associated to each output, where\n",
      " |           the weights are given by output_weight.\n",
      " |           Output weights must be greater than or equal\n",
      " |           to zero. Outputs with zero weight do not contribute at all\n",
      " |           to the total mean squared error (MSE).\n",
      " |      \n",
      " |      missing : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features.\n",
      " |          The missing parameter can be a numpy array, a pandas DataFrame, or\n",
      " |          a  patsy DesignMatrix.  All entries will be interpreted as boolean\n",
      " |          values, with True indicating the corresponding entry in X should be\n",
      " |          interpreted as missing.  If the missing argument not used but the X\n",
      " |          argument is a pandas DataFrame, missing will be inferred from X if\n",
      " |          allow_missing is True.\n",
      " |  \n",
      " |  predict(self, X, missing=None, skip_scrub=False)\n",
      " |       Predict the response based on the input data X.\n",
      " |      \n",
      " |      \n",
      " |       Parameters\n",
      " |       ----------\n",
      " |       X : array-like, shape = [m, n] where m is the number of samples and n\n",
      " |           is the number of features\n",
      " |           The training predictors.  The X parameter can be a numpy\n",
      " |           array, a pandas DataFrame, or a patsy DesignMatrix.\n",
      " |      \n",
      " |       missing : array-like, shape = [m, n] where m is the number of samples\n",
      " |           and n is the number of features.\n",
      " |           The missing parameter can be a numpy array, a pandas DataFrame, or\n",
      " |           a  patsy DesignMatrix.  All entries will be interpreted as boolean\n",
      " |           values, with True indicating the corresponding entry in X should be\n",
      " |           interpreted as missing.  If the missing argument not used but the X\n",
      " |           argument is a pandas DataFrame, missing will be inferred from X if\n",
      " |           allow_missing is True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |           y : array of shape = [m] or [m, p] where m is the number of samples\n",
      " |               and p is the number of outputs\n",
      " |               The predicted values.\n",
      " |  \n",
      " |  predict_deriv(self, X, variables=None, missing=None)\n",
      " |       Predict the first derivatives of the response based on the input\n",
      " |       data X.\n",
      " |      \n",
      " |      \n",
      " |       Parameters\n",
      " |       ----------\n",
      " |       X : array-like, shape = [m, n] where m is the number of samples and n\n",
      " |           is the number of features The training predictors. The X parameter\n",
      " |           can be a numpy array, a pandas DataFrame, or a patsy DesignMatrix.\n",
      " |      \n",
      " |       missing : array-like, shape = [m, n] where m is the number of samples\n",
      " |           and n is the number of features.\n",
      " |           The missing parameter can be a numpy array, a pandas DataFrame, or\n",
      " |           a patsy DesignMatrix.  All entries will be interpreted as boolean\n",
      " |           values, with True indicating the corresponding entry in X should be\n",
      " |           interpreted as missing.  If the missing argument not used but the X\n",
      " |           argument is a pandas DataFrame, missing will be inferred from X if\n",
      " |           allow_missing is True.\n",
      " |      \n",
      " |       variables : list\n",
      " |           The variables over which derivatives will be computed.  Each column\n",
      " |           in the resulting array corresponds to a variable.  If not\n",
      " |           specified, all variables are used (even if some are not relevant\n",
      " |           to the final model and have derivatives that are identically zero).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      X_deriv : array of shape = [m, n, p] where m is the number of samples, n\n",
      " |                is the number of features if 'variables' is not specified\n",
      " |                otherwise it is len(variables) and p is the number of outputs.\n",
      " |                For each sample, X_deriv represents the first derivative of\n",
      " |                each response  with respect to each variable.\n",
      " |  \n",
      " |  pruning_pass(self, X, y=None, sample_weight=None, output_weight=None, missing=None, skip_scrub=False)\n",
      " |      Perform the pruning pass of the multivariate adaptive regression\n",
      " |      splines algorithm.  Users will normally want to call the fit\n",
      " |      method instead, which performs the forward pass, the pruning\n",
      " |      pass, and a linear fit to determine the final model coefficients.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features The training predictors.\n",
      " |          The X parameter can be a numpy array, a pandas DataFrame, a patsy\n",
      " |          DesignMatrix, or a tuple of patsy DesignMatrix objects as output\n",
      " |          by patsy.dmatrices.\n",
      " |      \n",
      " |      y : array-like, optional (default=None), shape = [m, p] where m is the\n",
      " |          number of samples, p the number of outputs.\n",
      " |          The y parameter can be a numpy array, a pandas DataFrame,\n",
      " |          a Patsy DesignMatrix, or can be left as None (default) if X was\n",
      " |          the output of a call to patsy.dmatrices (in which case, X contains\n",
      " |          the response).\n",
      " |      \n",
      " |      sample_weight : array-like, optional (default=None), shape = [m]\n",
      " |           where m is the number of samples.\n",
      " |           Sample weights for training.  Weights must be greater than or\n",
      " |           equal to zero. Rows with zero weight do not contribute at all.\n",
      " |           Weights are useful when dealing with heteroscedasticity.  In such\n",
      " |           cases, the weight should be proportional to the inverse of the\n",
      " |           (known) variance.\n",
      " |      \n",
      " |      output_weight : array-like, optional (default=None), shape = [p]\n",
      " |           where p is the number of outputs.\n",
      " |           The total mean squared error (MSE) is a weighted sum of\n",
      " |           mean squared errors (MSE) associated to each output, where\n",
      " |           the weights are given by output_weight.\n",
      " |           Output weights must be greater than or equal\n",
      " |           to zero. Outputs with zero weight do not contribute at all\n",
      " |           to the total mean squared error (MSE).\n",
      " |      \n",
      " |      missing : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features.\n",
      " |          The missing parameter can be a numpy array, a pandas DataFrame, or\n",
      " |          a  patsy DesignMatrix.  All entries will be interpreted as boolean\n",
      " |          values, with True indicating the corresponding entry in X should be\n",
      " |          interpreted as missing.  If the missing argument not used but the X\n",
      " |          argument is a pandas DataFrame, missing will be inferred from X if\n",
      " |          allow_missing is True.\n",
      " |  \n",
      " |  pruning_trace(self)\n",
      " |      Return information about the pruning pass.\n",
      " |  \n",
      " |  score(self, X, y=None, sample_weight=None, output_weight=None, missing=None, skip_scrub=False)\n",
      " |      Calculate the generalized r^2 of the model on data X and y.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features The training predictors.\n",
      " |          The X parameter can be a numpy array, a pandas DataFrame, a patsy\n",
      " |          DesignMatrix, or a tuple of patsy DesignMatrix objects as output\n",
      " |          by patsy.dmatrices.\n",
      " |      \n",
      " |      y : array-like, optional (default=None), shape = [m, p] where m is the\n",
      " |          number of samples, p the number of outputs.\n",
      " |          The y parameter can be a numpy array, a pandas DataFrame,\n",
      " |          a Patsy DesignMatrix, or can be left as None (default) if X was\n",
      " |          the output of a call to patsy.dmatrices (in which case, X contains\n",
      " |          the response).\n",
      " |      \n",
      " |      sample_weight : array-like, optional (default=None), shape = [m]\n",
      " |           where m is the number of samples.\n",
      " |           Sample weights for training.  Weights must be greater than or\n",
      " |           equal to zero. Rows with zero weight do not contribute at all.\n",
      " |           Weights are useful when dealing with heteroscedasticity.  In such\n",
      " |           cases, the weight should be proportional to the inverse of the\n",
      " |           (known) variance.\n",
      " |      \n",
      " |      output_weight : array-like, optional (default=None), shape = [p]\n",
      " |           where p is the number of outputs.\n",
      " |           The total mean squared error (MSE) is a weighted sum of\n",
      " |           mean squared errors (MSE) associated to each output, where\n",
      " |           the weights are given by output_weight.\n",
      " |           Output weights must be greater than or equal\n",
      " |           to zero. Outputs with zero weight do not contribute at all\n",
      " |           to the total mean squared error (MSE).\n",
      " |      \n",
      " |      missing : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features.\n",
      " |          The missing parameter can be a numpy array, a pandas DataFrame, or\n",
      " |          a patsy DesignMatrix.  All entries will be interpreted as boolean\n",
      " |          values, with True indicating the corresponding entry in X should be\n",
      " |          interpreted as missing.  If the missing argument not used but the X\n",
      " |          argument is a pandas DataFrame, missing will be inferred from X if\n",
      " |          allow_missing is True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      score : float with a maximum value of 1 (it can be negative). The score\n",
      " |              is the generalized r^2 of the model on data X and y, the higher\n",
      " |              the score the better the fit is.\n",
      " |  \n",
      " |  score_samples(self, X, y=None, missing=None)\n",
      " |      Calculate sample-wise fit scores.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features The training predictors.\n",
      " |          The X parameter can be a numpy array, a pandas DataFrame, a patsy\n",
      " |          DesignMatrix, or a tuple of patsy DesignMatrix objects as output\n",
      " |          by patsy.dmatrices.\n",
      " |      \n",
      " |      y : array-like, optional (default=None), shape = [m, p] where m is the\n",
      " |          number of samples, p the number of outputs.\n",
      " |          The y parameter can be a numpy array, a pandas DataFrame,\n",
      " |          a Patsy DesignMatrix, or can be left as None (default) if X was\n",
      " |          the output of a call to patsy.dmatrices (in which case, X contains\n",
      " |          the response).\n",
      " |      \n",
      " |      missing : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features.\n",
      " |          The missing parameter can be a numpy array, a pandas DataFrame, or\n",
      " |          a  patsy DesignMatrix.  All entries will be interpreted as boolean\n",
      " |          values, with True indicating the corresponding entry in X should be\n",
      " |          interpreted as missing.  If the missing argument not used but the X\n",
      " |          argument is a pandas DataFrame, missing will be inferred from X if\n",
      " |          allow_missing is True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      scores : array of shape=[m, p] of floats with maximum value of 1\n",
      " |               (it can be negative).\n",
      " |               The scores represent how good each output of each example is\n",
      " |               predicted, a perfect score would be 1\n",
      " |               (the score can be negative).\n",
      " |  \n",
      " |  summary(self)\n",
      " |      Return a string describing the model.\n",
      " |  \n",
      " |  summary_feature_importances(self, sort_by=None)\n",
      " |      Returns a string containing a printable summary of the estimated\n",
      " |      feature importances.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sory_by : string, optional\n",
      " |          it refers to a feature importance type name : 'gcv', 'rss'\n",
      " |          or 'nb_subsets'.\n",
      " |          In case it is provided, the features are sorted\n",
      " |          according to the feature importance type corresponding\n",
      " |          to `sort_by`. In case it is not provided, the features are\n",
      " |          not sorted.\n",
      " |  \n",
      " |  trace(self)\n",
      " |      Return information about the forward and pruning passes.\n",
      " |  \n",
      " |  transform(self, X, missing=None)\n",
      " |      Transform X into the basis space.  Normally, users will call the\n",
      " |      predict method instead, which both transforms into basis space\n",
      " |      calculates the weighted sum of basis terms to produce a prediction\n",
      " |      of the response.  Users may wish to call transform directly in some\n",
      " |      cases.  For example, users may wish to apply other statistical or\n",
      " |      machine learning algorithms, such as generalized linear regression,\n",
      " |      in basis space.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [m, n] where m is the number of samples and n\n",
      " |          is the number of features\n",
      " |          The training predictors.  The X parameter can be a numpy array, a\n",
      " |          pandas DataFrame, or a patsy DesignMatrix.\n",
      " |      \n",
      " |      missing : array-like, shape = [m, n] where m is the number of samples\n",
      " |          and n is the number of features.\n",
      " |          The missing parameter can be a numpy array, a pandas DataFrame, or\n",
      " |          a patsy DesignMatrix.  All entries will be interpreted as boolean\n",
      " |          values, with True indicating the corresponding entry in X should be\n",
      " |          interpreted as missing.  If the missing argument not used but the X\n",
      " |          argument is a pandas DataFrame, missing will be inferred from X if\n",
      " |          allow_missing is True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      B: array of shape [m, nb_terms] where m is the number of samples and\n",
      " |         nb_terms is the number of terms (or basis functions) obtained after\n",
      " |         fitting (which is the number of elements of the attribute `basis_`).\n",
      " |         B represents the values of the basis functions evaluated at each\n",
      " |         sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  forward_pass_arg_names = {'allow_linear', 'allow_missing', 'check_ever...\n",
      " |  \n",
      " |  pruning_pass_arg_names = {'feature_importance_type', 'penalty', 'verbo...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Earth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:44:56.586635Z",
     "start_time": "2022-01-28T19:44:56.371658Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyearth\n",
    "from pyearth import Earth\n",
    "\n",
    "model = Earth(max_terms=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:38:40.632342Z",
     "start_time": "2022-01-27T18:38:23.936446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning forward pass\n",
      "---------------------------------------------------------------\n",
      "iter  parent  var  knot  mse       terms  gcv    rsq    grsq   \n",
      "---------------------------------------------------------------\n",
      "0     -       -    -     0.011780  1      0.012  0.000  0.000  \n",
      "1     0       399  2240  0.011636  3      0.012  0.012  0.011  \n",
      "2     0       348  1064  0.011010  5      0.011  0.065  0.063  \n",
      "3     0       391  336   0.010770  7      0.011  0.086  0.083  \n",
      "4     0       399  1210  0.010597  9      0.011  0.100  0.096  \n",
      "5     0       399  1376  0.010066  11     0.010  0.145  0.141  \n",
      "6     0       391  17    0.009518  13     0.010  0.192  0.186  \n",
      "7     0       348  27    0.009038  15     0.009  0.233  0.226  \n",
      "8     0       391  1377  0.008397  17     0.008  0.287  0.281  \n",
      "9     0       204  1925  0.008202  19     0.008  0.304  0.296  \n",
      "10    0       7    1308  0.008018  21     0.008  0.319  0.311  \n",
      "---------------------------------------------------------------\n",
      "Stopping Condition 0: Reached maximum number of terms\n",
      "Beginning pruning pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasia\\anaconda3\\envs\\myenv\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  pruning_passer.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "iter  bf  terms  mse   gcv    rsq    grsq   \n",
      "--------------------------------------------\n",
      "0     -   21     0.01  0.009  0.284  0.275  \n",
      "1     1   20     0.01  0.008  0.319  0.312  \n",
      "2     10  19     0.01  0.008  0.319  0.312  \n",
      "3     15  18     0.01  0.008  0.319  0.313  \n",
      "4     14  17     0.01  0.008  0.319  0.313  \n",
      "5     5   16     0.01  0.008  0.319  0.313  \n",
      "6     11  15     0.01  0.008  0.319  0.314  \n",
      "7     17  14     0.01  0.008  0.319  0.314  \n",
      "8     19  13     0.01  0.008  0.319  0.315  \n",
      "9     6   12     0.01  0.008  0.319  0.315  \n",
      "10    2   11     0.01  0.008  0.318  0.314  \n",
      "11    4   10     0.01  0.008  0.302  0.298  \n",
      "12    20  9      0.01  0.008  0.284  0.281  \n",
      "13    18  8      0.01  0.009  0.257  0.254  \n",
      "14    8   7      0.01  0.009  0.237  0.235  \n",
      "15    3   6      0.01  0.010  0.149  0.146  \n",
      "16    13  5      0.01  0.010  0.148  0.147  \n",
      "17    12  4      0.01  0.011  0.066  0.064  \n",
      "18    16  3      0.01  0.012  0.019  0.018  \n",
      "19    9   2      0.01  0.012  0.001  0.000  \n",
      "20    7   1      0.01  0.012  -0.000  -0.000  \n",
      "----------------------------------------------\n",
      "Selected iteration: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasia\\anaconda3\\envs\\myenv\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Earth(max_terms=20, verbose=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_2D, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:40:13.469161Z",
     "start_time": "2022-01-27T18:40:13.392204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLINES_RMSE: 0.5093975694449377\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test_2D)\n",
    "\n",
    "error = np.sqrt(mean_squared_error(y_test, predictions, multioutput='uniform_average'))\n",
    "print('SPLINES_RMSE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Strategies\n",
    "\n",
    "Our trading system can now open two types of positions: long and short. The first type is the long position.Long positions are opened by buying a commodity at time t and price p, and selling it at a later time t + x. The second type is the short position, where the trader sells the security at time t with price p with the obligation of buying it in the future.\n",
    "\n",
    "\n",
    "Our trading system can now open two types of positions: long and short. The first type is the long position.Long positions are opened by buying a commodity at time t and price p, and selling it at a later time t + x. The second type is the short position, where the trader sells the security at time t with price p with the obligation of buying it in the future.\n",
    "\n",
    "\n",
    "The mechanics of the first trading strategy we are going to use are the\n",
    "following. First, all decisions will be taken at the end of the day, that is, after\n",
    "knowing all daily quotes of the current session. Suppose that at the end of\n",
    "day t, our models provide evidence that the prices are going down, that is,\n",
    "predicting a low value of T or a sell signal. If we already have a position\n",
    "opened, the indication of the model will be ignored. If we currently do not\n",
    "hold any opened position, we will open a short position by issuing a sell order.\n",
    "When this order is carried out by the market at a price pr sometime in the\n",
    "future, we will immediately post two other orders. The first is a buy limit\n",
    "order with a limit price of pr − p%, where p% is a target profit margin. This\n",
    "type of order is carried out only if the market price reaches the target limit\n",
    "price or below. This order expresses what our target profit is for the short\n",
    "position just opened. We will wait 10 days for this target to be reached. If the\n",
    "order is not carried out by this deadline, we will buy at the closing price of\n",
    "the 10th day. The second order is a buy stop order with a price limit pr + l%.\n",
    "This order is placed with the goal of limiting our eventual losses with this\n",
    "position. The order will be executed if the market reaches the price pr + l%,\n",
    "thus limiting our possible losses to l%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Decision Making\n",
    "\n",
    "In our approach to this problem we will assume that the correct trading action at time t is related to what our expectations are concerning the evolution of prices in the next k days. Moreover, we will describe this future evolution of the prices by our indicator T. The correct trading signal at time t will be “buy” if the T score is higher than a certain threshold, and will be “sell” if the score is below another threshold. In all other cases, the correct signal will be do nothing (i.e., “hold”).\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  signal =\n",
    "    \\begin{cases}\n",
    "      sell & \\text{if if T < −0.1}\\\\\n",
    "      hold & \\text{if − 0.1 ≤ T ≤ 0.1}\\\\\n",
    "      buy & \\text{if T > 0.1}\n",
    "    \\end{cases}       \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split to Train and Test with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:25:38.253532Z",
     "start_time": "2022-01-28T19:25:37.851390Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:27:20.731302Z",
     "start_time": "2022-01-28T19:26:40.984896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "254/254 [==============================] - 3s 1ms/step - loss: 53.1984\n",
      "Epoch 2/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 1.3136\n",
      "Epoch 3/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.5693\n",
      "Epoch 4/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.3007\n",
      "Epoch 5/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2622\n",
      "Epoch 6/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2468\n",
      "Epoch 7/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2244\n",
      "Epoch 8/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2230\n",
      "Epoch 9/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2268\n",
      "Epoch 10/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2273\n",
      "Epoch 11/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2185\n",
      "Epoch 12/100\n",
      "254/254 [==============================] - 1s 2ms/step - loss: 0.2389\n",
      "Epoch 13/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2227\n",
      "Epoch 14/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2257\n",
      "Epoch 15/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.5319\n",
      "Epoch 16/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2536\n",
      "Epoch 17/100\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.5431\n",
      "Epoch 18/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2693\n",
      "Epoch 19/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 2.4903\n",
      "Epoch 20/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.3715\n",
      "Epoch 21/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2213\n",
      "Epoch 22/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2245\n",
      "Epoch 23/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2861\n",
      "Epoch 24/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 3.7617\n",
      "Epoch 25/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.4633\n",
      "Epoch 26/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2228\n",
      "Epoch 27/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2105\n",
      "Epoch 28/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2217\n",
      "Epoch 29/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 1.1194\n",
      "Epoch 30/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.4704\n",
      "Epoch 31/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2214\n",
      "Epoch 32/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2514\n",
      "Epoch 33/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.3853\n",
      "Epoch 34/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 3.7012\n",
      "Epoch 35/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2327\n",
      "Epoch 36/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2121\n",
      "Epoch 37/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2095\n",
      "Epoch 38/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2093\n",
      "Epoch 39/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2105\n",
      "Epoch 40/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.6831\n",
      "Epoch 41/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.8341\n",
      "Epoch 42/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2117\n",
      "Epoch 43/100\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.3519\n",
      "Epoch 44/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.3620\n",
      "Epoch 45/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.4707\n",
      "Epoch 46/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 3.1498\n",
      "Epoch 47/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2006\n",
      "Epoch 48/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2064\n",
      "Epoch 49/100\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2058\n",
      "Epoch 50/100\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2810\n",
      "Epoch 51/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.5068\n",
      "Epoch 52/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2368\n",
      "Epoch 53/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.3600\n",
      "Epoch 54/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 2.7156\n",
      "Epoch 55/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2036\n",
      "Epoch 56/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2013\n",
      "Epoch 57/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2081\n",
      "Epoch 58/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2285\n",
      "Epoch 59/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2604\n",
      "Epoch 60/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 1.3617\n",
      "Epoch 61/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2024\n",
      "Epoch 62/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.5553\n",
      "Epoch 63/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2725\n",
      "Epoch 64/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.5915\n",
      "Epoch 65/100\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 3.6138\n",
      "Epoch 66/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2420\n",
      "Epoch 67/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2034\n",
      "Epoch 68/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2029\n",
      "Epoch 69/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2009\n",
      "Epoch 70/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2100\n",
      "Epoch 71/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.3046\n",
      "Epoch 72/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.4762\n",
      "Epoch 73/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.3287\n",
      "Epoch 74/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 4.3849A: 0s - loss: 0.\n",
      "Epoch 75/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2378\n",
      "Epoch 76/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.1982\n",
      "Epoch 77/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.1975\n",
      "Epoch 78/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2056\n",
      "Epoch 79/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.1999\n",
      "Epoch 80/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2083\n",
      "Epoch 81/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2212\n",
      "Epoch 82/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2044\n",
      "Epoch 83/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2437\n",
      "Epoch 84/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 2.5800\n",
      "Epoch 85/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.1997\n",
      "Epoch 86/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2164\n",
      "Epoch 87/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2037\n",
      "Epoch 88/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2066\n",
      "Epoch 89/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.4006\n",
      "Epoch 90/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 1.1715\n",
      "Epoch 91/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2628\n",
      "Epoch 92/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2011\n",
      "Epoch 93/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2036\n",
      "Epoch 94/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.6111\n",
      "Epoch 95/100\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.5617\n",
      "Epoch 96/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.9433\n",
      "Epoch 97/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2642\n",
      "Epoch 98/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2101\n",
      "Epoch 99/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2053\n",
      "Epoch 100/100\n",
      "254/254 [==============================] - 0s 1ms/step - loss: 0.2940\n"
     ]
    }
   ],
   "source": [
    "# Initialising the MLP\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "MLPmodel = Sequential()\n",
    "MLPmodel.add(Dense(100, activation='relu', input_dim=x_train.shape[1]))\n",
    "MLPmodel.add(Dense(1))\n",
    "MLPmodel.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "MLPmodel.fit(x_train, y_train, epochs=100, verbose=1)\n",
    "\n",
    "# check predicted values\n",
    "predictions = MLPmodel.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:27:22.538721Z",
     "start_time": "2022-01-28T19:27:22.530721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_RMSE: 2.9104190687872618\n"
     ]
    }
   ],
   "source": [
    "error = np.sqrt(mean_squared_error(y_test, predictions, multioutput='uniform_average'))\n",
    "print('MLP_RMSE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:38:43.024240Z",
     "start_time": "2022-01-28T19:37:34.433701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "17/17 [==============================] - 18s 741ms/step - loss: 0.2999\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - 12s 709ms/step - loss: 0.2084\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - 12s 711ms/step - loss: 0.2057\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - 13s 743ms/step - loss: 0.2045\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - 12s 728ms/step - loss: 0.2033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x243e3593940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import LSTM, Dropout\n",
    "\n",
    "# Initialising the LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=200, return_sequences=True, activation='relu', input_shape=(x_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and Dropout layer\n",
    "model.add(LSTM(units=200,activation='relu', return_sequences=True))\n",
    "\n",
    "# Adding a third LSTM layer and Dropout layer\n",
    "model.add(LSTM(units=200, return_sequences=True,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and and Dropout layer\n",
    "model.add(LSTM(units=200))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "# For Full connection layer we use dense\n",
    "# As the output is 1D so we use unit=1\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# compile and fit the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:39:51.108609Z",
     "start_time": "2022-01-28T19:39:51.092589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_RMSE: 2.9104190687872618\n"
     ]
    }
   ],
   "source": [
    "error = np.sqrt(mean_squared_error(y_test, predictions, multioutput='uniform_average'))\n",
    "print('LSTM_RMSE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:40:15.550039Z",
     "start_time": "2022-01-28T19:39:54.192276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "254/254 [==============================] - 1s 2ms/step - loss: 3.1168\n",
      "Epoch 2/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2189\n",
      "Epoch 3/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2780\n",
      "Epoch 4/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2482\n",
      "Epoch 5/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 1.3720\n",
      "Epoch 6/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.5303\n",
      "Epoch 7/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 1.8879\n",
      "Epoch 8/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 3.0550\n",
      "Epoch 9/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2161\n",
      "Epoch 10/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2114\n",
      "Epoch 11/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2105\n",
      "Epoch 12/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 13/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2545\n",
      "Epoch 14/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2259\n",
      "Epoch 15/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2657\n",
      "Epoch 16/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.3566\n",
      "Epoch 17/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.6083\n",
      "Epoch 18/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 1.3227\n",
      "Epoch 19/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2231\n",
      "Epoch 20/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2165\n",
      "Epoch 21/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.8933\n",
      "Epoch 22/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.5039\n",
      "Epoch 23/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2160\n",
      "Epoch 24/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2161\n",
      "Epoch 25/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2239\n",
      "Epoch 26/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2522\n",
      "Epoch 27/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2605\n",
      "Epoch 28/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 1.4339\n",
      "Epoch 29/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2289\n",
      "Epoch 30/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2616\n",
      "Epoch 31/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2160\n",
      "Epoch 32/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2265\n",
      "Epoch 33/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2497A: 0s - loss:\n",
      "Epoch 34/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2957\n",
      "Epoch 35/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2404\n",
      "Epoch 36/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2331\n",
      "Epoch 37/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.7594\n",
      "Epoch 38/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2106\n",
      "Epoch 39/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2253\n",
      "Epoch 40/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2109\n",
      "Epoch 41/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.5352\n",
      "Epoch 42/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2216\n",
      "Epoch 43/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.4486\n",
      "Epoch 44/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2596\n",
      "Epoch 45/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2159\n",
      "Epoch 46/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2334\n",
      "Epoch 47/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.3961\n",
      "Epoch 48/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.3471\n",
      "Epoch 49/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2256\n",
      "Epoch 50/50\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.2386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x243e8af6a60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "modelCNN = Sequential()\n",
    "modelCNN.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(x_train.shape[1], 1)))\n",
    "modelCNN.add(MaxPooling1D(pool_size=2))\n",
    "modelCNN.add(Flatten())\n",
    "modelCNN.add(Dense(50, activation='relu'))\n",
    "modelCNN.add(Dense(1))\n",
    "modelCNN.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "modelCNN.fit(x_train, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:40:54.490987Z",
     "start_time": "2022-01-28T19:40:54.088270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_RMSE: 0.19451442039293135\n"
     ]
    }
   ],
   "source": [
    "predictions = modelCNN.predict(x_test)\n",
    "\n",
    "error = np.sqrt(mean_squared_error(y_test, predictions, multioutput='uniform_average'))\n",
    "print('CNN_RMSE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:43:48.488239Z",
     "start_time": "2022-01-28T19:43:48.157112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_RMSE: 0.057947135804246666\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "sv = svm.SVR()\n",
    "sv.fit(x_train, y_train)\n",
    "y_pred = sv.predict(x_test)\n",
    "\n",
    "error = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('CNN_RMSE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:44:25.470574Z",
     "start_time": "2022-01-28T19:44:25.462554Z"
    }
   },
   "source": [
    "## SPLINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:46:07.807682Z",
     "start_time": "2022-01-28T19:46:07.791660Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Earth(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T19:47:39.694318Z",
     "start_time": "2022-01-28T19:46:08.099302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning forward pass\n",
      "---------------------------------------------------------------\n",
      "iter  parent  var  knot  mse       terms  gcv    rsq    grsq   \n",
      "---------------------------------------------------------------\n",
      "0     -       -    -     0.206622  1      0.207  0.000  0.000  \n",
      "1     0       7    619   0.199500  3      0.200  0.034  0.033  \n",
      "2     0       4    1325  0.191263  5      0.192  0.074  0.072  \n",
      "3     0       4    2564  0.184093  7      0.185  0.109  0.106  \n",
      "4     0       4    160   0.174032  9      0.175  0.158  0.154  \n",
      "5     0       7    373   0.170895  11     0.172  0.173  0.168  \n",
      "6     0       7    1079  0.166465  13     0.168  0.194  0.188  \n",
      "7     0       7    1062  0.157974  15     0.159  0.235  0.229  \n",
      "8     0       7    1390  0.146847  17     0.148  0.289  0.282  \n",
      "9     0       7    1454  0.145301  19     0.147  0.297  0.289  \n",
      "10    0       4    1349  0.139539  21     0.141  0.325  0.316  \n",
      "11    0       7    16    0.133962  23     0.136  0.352  0.343  \n",
      "12    0       7    2003  0.130915  25     0.133  0.366  0.357  \n",
      "13    0       4    222   0.129919  27     0.132  0.371  0.361  \n",
      "14    0       4    38    0.128127  29     0.130  0.380  0.369  \n",
      "15    0       4    24    0.121567  31     0.124  0.412  0.401  \n",
      "16    0       4    1427  0.117294  33     0.120  0.432  0.421  \n",
      "17    0       4    1445  0.116209  35     0.119  0.438  0.426  \n",
      "18    0       7    6918  0.115558  37     0.118  0.441  0.428  \n",
      "19    0       4    2058  0.113542  39     0.116  0.450  0.437  \n",
      "20    0       4    54    0.110771  41     0.114  0.464  0.450  \n",
      "21    0       7    1098  0.107378  43     0.110  0.480  0.467  \n",
      "22    0       4    173   0.104536  45     0.107  0.494  0.480  \n",
      "23    0       4    7385  0.101200  47     0.104  0.510  0.496  \n",
      "24    0       4    1455  0.096135  49     0.099  0.535  0.521  \n",
      "25    0       4    7081  0.093855  51     0.097  0.546  0.531  \n",
      "26    0       7    53    0.093028  53     0.096  0.550  0.535  \n",
      "27    0       4    37    0.092539  55     0.096  0.552  0.537  \n",
      "28    0       4    7193  0.091711  57     0.095  0.556  0.540  \n",
      "29    0       4    7222  0.088735  59     0.092  0.571  0.555  \n",
      "30    0       4    232   0.087614  61     0.091  0.576  0.560  \n",
      "31    0       7    1376  0.087283  63     0.091  0.578  0.561  \n",
      "32    0       4    1474  0.086578  65     0.090  0.581  0.564  \n",
      "33    0       7    31    0.085732  67     0.089  0.585  0.568  \n",
      "34    0       7    2042  0.084883  69     0.089  0.589  0.571  \n",
      "35    0       7    2052  0.084601  71     0.088  0.591  0.572  \n",
      "36    0       4    7281  0.084098  73     0.088  0.593  0.574  \n",
      "37    0       7    88    0.082666  75     0.087  0.600  0.581  \n",
      "38    0       4    120   0.082299  77     0.086  0.602  0.582  \n",
      "39    0       7    1413  0.081673  79     0.086  0.605  0.585  \n",
      "40    0       7    60    0.081472  81     0.086  0.606  0.586  \n",
      "---------------------------------------------------------------\n",
      "Stopping Condition 2: Improvement below threshold\n",
      "Beginning pruning pass\n",
      "--------------------------------------------\n",
      "iter  bf  terms  mse   gcv    rsq    grsq   \n",
      "--------------------------------------------\n",
      "0     -   81     0.14  0.145  0.334  0.300  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasia\\anaconda3\\envs\\myenv\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  pruning_passer.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     71  80     0.14  0.144  0.335  0.301  \n",
      "2     49  79     0.14  0.144  0.335  0.302  \n",
      "3     11  78     0.14  0.144  0.335  0.302  \n",
      "4     50  77     0.14  0.144  0.335  0.302  \n",
      "5     51  76     0.14  0.144  0.335  0.303  \n",
      "6     80  75     0.14  0.144  0.335  0.303  \n",
      "7     56  74     0.14  0.144  0.335  0.304  \n",
      "8     4   73     0.14  0.144  0.335  0.304  \n",
      "9     62  72     0.14  0.144  0.335  0.305  \n",
      "10    68  71     0.14  0.144  0.335  0.305  \n",
      "11    70  70     0.14  0.144  0.335  0.306  \n",
      "12    65  69     0.14  0.143  0.335  0.306  \n",
      "13    76  68     0.14  0.143  0.335  0.306  \n",
      "14    34  67     0.14  0.143  0.335  0.307  \n",
      "15    17  66     0.14  0.143  0.335  0.307  \n",
      "16    48  65     0.14  0.143  0.335  0.308  \n",
      "17    9   64     0.14  0.143  0.335  0.308  \n",
      "18    30  63     0.14  0.143  0.335  0.309  \n",
      "19    31  62     0.14  0.143  0.335  0.309  \n",
      "20    57  61     0.14  0.143  0.335  0.309  \n",
      "21    24  60     0.14  0.143  0.335  0.310  \n",
      "22    54  59     0.14  0.143  0.335  0.310  \n",
      "23    74  58     0.14  0.142  0.335  0.311  \n",
      "24    21  57     0.14  0.142  0.335  0.311  \n",
      "25    41  56     0.14  0.142  0.335  0.312  \n",
      "26    25  55     0.14  0.142  0.335  0.312  \n",
      "27    16  54     0.14  0.142  0.335  0.312  \n",
      "28    5   53     0.14  0.142  0.335  0.313  \n",
      "29    8   52     0.14  0.142  0.335  0.313  \n",
      "30    63  51     0.14  0.142  0.335  0.314  \n",
      "31    1   50     0.14  0.142  0.335  0.314  \n",
      "32    13  49     0.14  0.142  0.335  0.315  \n",
      "33    20  48     0.14  0.142  0.335  0.315  \n",
      "34    40  47     0.14  0.141  0.335  0.315  \n",
      "35    77  46     0.14  0.141  0.335  0.316  \n",
      "36    43  45     0.14  0.141  0.335  0.316  \n",
      "37    37  44     0.14  0.141  0.335  0.317  \n",
      "38    45  43     0.14  0.141  0.335  0.317  \n",
      "39    3   42     0.14  0.141  0.336  0.319  \n",
      "40    10  41     0.14  0.139  0.342  0.326  \n",
      "41    58  40     0.14  0.139  0.342  0.326  \n",
      "42    29  39     0.14  0.139  0.342  0.327  \n",
      "43    72  38     0.14  0.139  0.342  0.327  \n",
      "44    55  37     0.14  0.139  0.342  0.328  \n",
      "45    46  36     0.14  0.139  0.342  0.328  \n",
      "46    67  35     0.14  0.139  0.342  0.328  \n",
      "47    19  34     0.14  0.139  0.342  0.329  \n",
      "48    79  33     0.14  0.139  0.342  0.329  \n",
      "49    73  32     0.14  0.139  0.341  0.329  \n",
      "50    78  31     0.14  0.139  0.340  0.328  \n",
      "51    2   30     0.14  0.140  0.337  0.325  \n",
      "52    33  29     0.14  0.139  0.337  0.325  \n",
      "53    12  28     0.14  0.139  0.339  0.327  \n",
      "54    53  27     0.14  0.139  0.338  0.327  \n",
      "55    75  26     0.14  0.139  0.337  0.327  \n",
      "56    59  25     0.14  0.141  0.329  0.319  \n",
      "57    23  24     0.14  0.141  0.328  0.318  \n",
      "58    42  23     0.14  0.141  0.327  0.318  \n",
      "59    60  22     0.14  0.141  0.325  0.316  \n",
      "60    18  21     0.14  0.142  0.324  0.315  \n",
      "61    47  20     0.14  0.142  0.321  0.313  \n",
      "62    44  19     0.14  0.142  0.321  0.313  \n",
      "63    52  18     0.14  0.142  0.318  0.311  \n",
      "64    69  17     0.14  0.143  0.316  0.309  \n",
      "65    27  16     0.14  0.143  0.313  0.306  \n",
      "66    32  15     0.14  0.144  0.311  0.305  \n",
      "67    22  14     0.14  0.144  0.307  0.301  \n",
      "68    14  13     0.14  0.145  0.302  0.297  \n",
      "69    6   12     0.15  0.147  0.294  0.289  \n",
      "70    64  11     0.15  0.149  0.283  0.278  \n",
      "71    38  10     0.15  0.149  0.282  0.278  \n",
      "72    26  9      0.15  0.155  0.255  0.251  \n",
      "73    28  8      0.15  0.155  0.255  0.251  \n",
      "74    7   7      0.16  0.160  0.230  0.227  \n",
      "75    39  6      0.16  0.160  0.230  0.228  \n",
      "76    15  5      0.17  0.171  0.176  0.174  \n",
      "77    66  4      0.20  0.205  0.008  0.006  \n",
      "78    35  3      0.21  0.206  0.007  0.005  \n",
      "79    61  2      0.21  0.206  0.005  0.005  \n",
      "80    36  1      0.21  0.207  0.000  0.000  \n",
      "--------------------------------------------\n",
      "Selected iteration: 47\n",
      "SPLINES_RMSE: 0.07551734130578028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasia\\anaconda3\\envs\\myenv\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train)\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "error = np.sqrt(mean_squared_error(y_test, predictions, multioutput='uniform_average'))\n",
    "print('SPLINES_RMSE:', error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
